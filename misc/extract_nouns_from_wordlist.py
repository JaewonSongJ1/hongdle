import re
from pathlib import Path
from typing import Set
import argparse
import sys

class NounExtractor:
    """
    'í•œêµ­ì–´ í•™ìŠµìš© ì–´íœ˜ ëª©ë¡' í˜•ì‹ì˜ íŒŒì¼ì—ì„œ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ê³  ì •ì œí•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self):
        pass

    def clean_word(self, word: str) -> str:
        """
        ë‹¨ì–´ì—ì„œ ìˆ«ìì™€ ë¶ˆí•„ìš”í•œ ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        - "ê°€ê²©03" -> "ê°€ê²©"
        - "ê°€-ë‹¿ë‹¤" -> "ê°€ë‹¿ë‹¤"
        """
        if not isinstance(word, str):
            return ""
        # ë‹¨ì–´ ëì— ë¶™ì€ ìˆ«ì ì œê±°
        cleaned = re.sub(r'\d+$', '', word)
        # í•œê¸€ì´ ì•„ë‹Œ ë¬¸ì ì œê±° (í•˜ì´í”ˆì€ í—ˆìš© í›„ ì œê±°)
        cleaned = cleaned.replace('-', '')
        cleaned = re.sub(r'[^ê°€-í£]', '', cleaned)
        return cleaned.strip()

    def extract_nouns(self, input_file: str, output_file: str):
        """
        íŒŒì¼ì„ ì½ì–´ 'ëª…ì‚¬' í’ˆì‚¬ì˜ ë‹¨ì–´ë§Œ ì¶”ì¶œí•˜ê³  ì •ì œí•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        """
        input_path = Path(input_file)
        if not input_path.exists():
            print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ '{input_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        print(f"ğŸ“– ì…ë ¥ íŒŒì¼: {input_path}")
        
        nouns: Set[str] = set()
        total_lines = 0
        noun_lines = 0

        # Windowsì—ì„œ ìƒì„±ëœ í•œê¸€ í…ìŠ¤íŠ¸ íŒŒì¼ì€ 'cp949' ì¸ì½”ë”©ì¸ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
        with open(input_path, 'r', encoding='cp949') as f:
            # í—¤ë” ë¼ì¸ ê±´ë„ˆë›°ê¸°
            next(f, None)
            
            for line in f:
                total_lines += 1
                # íƒ­ ë˜ëŠ” ì—¬ëŸ¬ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
                parts = re.split(r'\s+', line.strip())
                
                if len(parts) < 3:
                    continue
                
                word_raw = parts[1]
                pos = parts[2]

                if pos == 'ëª…':
                    noun_lines += 1
                    cleaned_word = self.clean_word(word_raw)
                    if cleaned_word:
                        nouns.add(cleaned_word)

        sorted_nouns = sorted(list(nouns))
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for noun in sorted_nouns:
                f.write(f"{noun}\n")

        print("\nğŸ‰ ì •ì œ ì™„ë£Œ!")
        print(f"  - ì´ ë¼ì¸ ìˆ˜: {total_lines:,}ê°œ")
        print(f"  - 'ëª…ì‚¬' í’ˆì‚¬ ë¼ì¸: {noun_lines:,}ê°œ")
        print(f"  - ì¶”ì¶œëœ ê³ ìœ  ëª…ì‚¬: {len(sorted_nouns):,}ê°œ")
        print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {output_path}")

def main():
    extractor = NounExtractor()
    project_root = Path(__file__).parent.parent
    input_file_path = project_root / 'data' / 'korean_study_word_list.txt'
    output_file_path = project_root / 'data' / 'korean_word_clean.txt'
    extractor.extract_nouns(str(input_file_path), str(output_file_path))

if __name__ == '__main__':
    main()