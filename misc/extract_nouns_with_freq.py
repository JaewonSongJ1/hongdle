import re
from pathlib import Path
import sys
from collections import Counter

class NounFrequencyExtractor:
    """
    'í•œêµ­ì–´ ì–´íœ˜ ëª©ë¡'ì—ì„œ ëª…ì‚¬ì™€ ë¹ˆë„ë¥¼ ì¶”ì¶œí•˜ê³  ì •ì œí•˜ëŠ” í´ë˜ìŠ¤.
    ë™ì¼í•œ ë‹¨ì–´(ì˜ˆ: ê°€01, ê°€07 -> ê°€)ì˜ ë¹ˆë„ë¥¼ í•©ì‚°í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        pass

    def clean_word(self, word: str) -> str:
        """ë‹¨ì–´ì—ì„œ ìˆ«ìì™€ ë¶ˆí•„ìš”í•œ ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        if not isinstance(word, str):
            return ""
        # ë‹¨ì–´ ëì— ë¶™ì€ ìˆ«ì ì œê±°
        cleaned = re.sub(r'\d+$', '', word)
        # í•œê¸€ì´ ì•„ë‹Œ ë¬¸ì ì œê±°
        cleaned = re.sub(r'[^ê°€-í£]', '', cleaned)
        return cleaned.strip()

    def extract(self, input_file: str, output_file: str):
        """íŒŒì¼ì„ ì½ì–´ ëª…ì‚¬ì™€ ë¹ˆë„ë¥¼ ì¶”ì¶œí•˜ê³  ì •ì œí•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
        input_path = Path(input_file)
        if not input_path.exists():
            print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ '{input_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

        print(f"ğŸ“– ì…ë ¥ íŒŒì¼: {input_path}")
        
        noun_freqs = Counter()
        total_lines = 0
        noun_lines = 0

        # Windows í™˜ê²½ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ ì¸ì½”ë”©(cp949)ì„ ê³ ë ¤í•©ë‹ˆë‹¤.
        # ë””ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ë¬¸ìëŠ” ë¬´ì‹œí•˜ê³  ì§„í–‰í•©ë‹ˆë‹¤.
        with open(input_path, 'r', encoding='cp949', errors='ignore') as f:
            # í—¤ë” ë¼ì¸(ì²« ì¤„)ì„ ì½ì–´ ì»¬ëŸ¼ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            header_line = f.readline().strip()
            header = re.split(r'\s+', header_line)
            
            try:
                word_idx = header.index('í•­ëª©')
                pos_idx = header.index('í’ˆì‚¬')
                freq_idx = header.index('ë¹ˆë„')
            except ValueError as e:
                print(f"âŒ ì˜¤ë¥˜: í—¤ë”ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                print(f"   ì°¾ì€ í—¤ë”: {header}")
                sys.exit(1)

            # ë°ì´í„° ë¼ì¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            for line in f:
                total_lines += 1
                parts = re.split(r'\s+', line.strip())
                
                if len(parts) <= max(word_idx, pos_idx, freq_idx):
                    continue

                if parts[pos_idx].strip() == 'ëª…':
                    noun_lines += 1
                    cleaned_word = self.clean_word(parts[word_idx])
                    try:
                        frequency = int(parts[freq_idx].strip())
                        if cleaned_word:
                            noun_freqs[cleaned_word] += frequency
                    except (ValueError, IndexError):
                        continue # ë¹ˆë„ ê°’ì´ ìˆ«ìê°€ ì•„ë‹ˆë©´ ë¬´ì‹œ
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•©ë‹ˆë‹¤.
        sorted_nouns = sorted(noun_freqs.items(), key=lambda item: item[1], reverse=True)

        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for word, freq in sorted_nouns:
                f.write(f"{word} {freq}\n")

        print("\nğŸ‰ ì •ì œ ì™„ë£Œ!")
        print(f"  - ì´ {noun_lines:,}ê°œì˜ ëª…ì‚¬ í•­ëª© ì²˜ë¦¬")
        print(f"  - ê³ ìœ  ëª…ì‚¬ ìˆ˜: {len(sorted_nouns):,}ê°œ")
        print(f"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {output_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    extractor = NounFrequencyExtractor()
    project_root = Path(__file__).parent.parent
    input_file_path = project_root / 'data' / 'korean_word_list.txt'
    output_file_path = project_root / 'data' / 'korean_word_clean_list.txt' # ìš”ì²­í•˜ì‹  íŒŒì¼ëª…
    extractor.extract(str(input_file_path), str(output_file_path))

if __name__ == '__main__':
    main()